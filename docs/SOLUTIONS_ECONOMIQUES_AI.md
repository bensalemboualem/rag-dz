# ğŸ’° Solutions Ã‰conomiques pour AI Providers

**Date**: 2025-01-20
**Objectif**: Trouver la solution la moins chÃ¨re pour Bolt + BMAD Agents

---

## ğŸ“Š Comparaison des CoÃ»ts (par 1M tokens)

| Provider | Input | Output | Performance | Free Tier | Recommandation |
|----------|-------|--------|-------------|-----------|----------------|
| **Groq** | **GRATUIT** | **GRATUIT** | Ultra rapide (500 tok/s) | 14,400 req/day | âœ… **MEILLEUR CHOIX** |
| **Cohere** | $0.15 | $0.60 | Rapide | 1000 req/mois | âœ… Bon backup |
| **DeepSeek** | $0.14 | $0.28 | Correct | Non | âœ… DÃ©jÃ  utilisÃ© |
| **OpenRouter** | Variable | Variable | Routage intelligent | Credits gratuits | âš ï¸ DÃ©pend du model |
| **Together** | $0.20 | $0.20 | Rapide | $25 credits | âš ï¸ Payant |
| **Gemini** | $0.075 | $0.30 | Bon | 15 req/min gratuit | âš ï¸ Rate limited |
| **Mistral** | $0.25 | $0.25 | Correct | Non | âŒ Payant |
| **Claude** | $3.00 | $15.00 | Excellent mais cher | Non | âŒ TRÃˆS CHER |
| **OpenAI** | $0.15-5.00 | $0.60-15.00 | Excellent mais cher | Non | âŒ CHER |

---

## ğŸ¯ SOLUTION RECOMMANDÃ‰E

### Architecture Ã  2 niveaux:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BOLT.DIY (Frontend)             â”‚
â”‚  Provider: GROQ (GRATUIT)               â”‚
â”‚  Model: llama-3.3-70b-versatile         â”‚
â”‚  Usage: GÃ©nÃ©ration de code Bolt         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    BACKEND BMAD (Agents Experts)        â”‚
â”‚  Provider: DEEPSEEK ($0.14/$0.28)       â”‚
â”‚  Model: deepseek-chat                   â”‚
â”‚  Usage: Conversations agents BMAD       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pourquoi cette solution?

1. **GROQ pour Bolt** (Frontend) âœ…
   - **100% GRATUIT** (14,400 requÃªtes/jour)
   - **Ultra rapide** (500 tokens/seconde)
   - ModÃ¨les puissants: Llama 3.3 70B, Mixtral 8x7B
   - Parfait pour gÃ©nÃ©ration de code

2. **DEEPSEEK pour BMAD** (Backend) âœ…
   - **TrÃ¨s Ã©conomique** ($0.14 input, $0.28 output)
   - Bon pour conversations (agents)
   - DÃ©jÃ  configurÃ© et testÃ©
   - 20x moins cher que Claude

**CoÃ»t estimÃ© mensuel**: **~$5-10** (seulement BMAD agents)

---

## ğŸ”§ Configuration GROQ pour Bolt

### Ã‰tape 1: VÃ©rifier la ClÃ© Groq

Tu as dÃ©jÃ  la clÃ© dans `.env.local`:
```env
GROQ_API_KEY=gsk_YOUR_GROQ_API_KEY_HERE
```

### Ã‰tape 2: Configurer Bolt pour Groq

1. Ouvre Bolt: http://localhost:5174
2. Clique sur **Settings** (âš™ï¸)
3. Section "**Provider**": SÃ©lectionne **Groq**
4. Section "**Model**": SÃ©lectionne **llama-3.3-70b-versatile**
5. Ferme les settings

**ModÃ¨les Groq disponibles**:
- `llama-3.3-70b-versatile` (Meilleur, 128k context)
- `llama-3.1-70b-versatile` (TrÃ¨s bon)
- `mixtral-8x7b-32768` (Rapide)
- `gemma2-9b-it` (LÃ©ger)

### Ã‰tape 3: Tester

1. Tape un message dans Bolt (sans agent BMAD)
2. VÃ©rifie que Groq rÃ©pond rapidement
3. Si erreur, vÃ©rifie les logs:
   ```bash
   docker logs ragdz-bolt-diy -f
   ```

---

## ğŸš€ Configuration OLLAMA Local (Pour VPS)

### Architecture VPS avec Ollama

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VPS SERVER                 â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Docker Container: Ollama         â”‚ â”‚
â”‚  â”‚  - llama3.2:3b (lÃ©ger, rapide)    â”‚ â”‚
â”‚  â”‚  - deepseek-r1:7b (reasoning)     â”‚ â”‚
â”‚  â”‚  - qwen2.5-coder:7b (code)        â”‚ â”‚
â”‚  â”‚  Port: 11434                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Backend RAG.dz (Port 8180)       â”‚ â”‚
â”‚  â”‚  - BMAD Agents â†’ Ollama local     â”‚ â”‚
â”‚  â”‚  - Pas de coÃ»t API externe!       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Bolt.DIY (Port 5174)             â”‚ â”‚
â”‚  â”‚  - Provider: Groq (gratuit)       â”‚ â”‚
â”‚  â”‚  - Ou Ollama local en backup      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ã‰tape 1: Installer Ollama sur VPS

```bash
# Sur ton VPS
curl -fsSL https://ollama.com/install.sh | sh

# Ou via Docker (recommandÃ©)
docker run -d \
  --name ollama \
  --gpus all \
  -v ollama_data:/root/.ollama \
  -p 11434:11434 \
  ollama/ollama
```

### Ã‰tape 2: TÃ©lÃ©charger ModÃ¨les

```bash
# ModÃ¨les recommandÃ©s pour BMAD agents
docker exec ollama ollama pull llama3.2:3b       # 2GB, rapide
docker exec ollama ollama pull qwen2.5-coder:7b  # 4GB, bon pour code
docker exec ollama ollama pull deepseek-r1:7b    # 4GB, reasoning

# Alternative Ã©conomique
docker exec ollama ollama pull gemma2:2b         # 1.5GB, trÃ¨s lÃ©ger
```

### Ã‰tape 3: Configurer Backend pour Ollama

Modifie `backend/rag-compat/app/routers/bmad_chat.py`:

```python
def get_ollama_client():
    """Client Ollama local"""
    from openai import OpenAI

    ollama_url = os.getenv("OLLAMA_API_BASE_URL", "http://ollama:11434/v1")

    return OpenAI(
        api_key="ollama",  # Ollama n'a pas besoin de vraie clÃ©
        base_url=ollama_url
    )

@router.post("/chat")
async def chat_with_agent(request: ChatRequest):
    # Choisir le provider
    use_ollama = os.getenv("USE_OLLAMA", "false").lower() == "true"

    if use_ollama:
        client = get_ollama_client()
        model = "llama3.2:3b"  # Ou autre
    else:
        client = get_deepseek_client()
        model = "deepseek-chat"

    # Appel API identique
    response = client.chat.completions.create(
        model=model,
        messages=messages
    )
```

### Ã‰tape 4: Variables d'Environnement VPS

```env
# .env sur VPS
USE_OLLAMA=true
OLLAMA_API_BASE_URL=http://ollama:11434/v1

# Backup Groq si Ollama down
GROQ_API_KEY=gsk_YOUR_GROQ_API_KEY_HERE
```

---

## ğŸ“‹ Plan de Migration VPS

### Option A: 100% Gratuit avec Ollama Local

**Avantages**:
- âœ… CoÃ»t API: **$0/mois**
- âœ… Pas de rate limits
- âœ… DonnÃ©es privÃ©es (sur ton serveur)
- âœ… Latence basse

**InconvÃ©nients**:
- âŒ Besoin GPU (ou CPU puissant)
- âŒ Plus lent que cloud APIs
- âŒ Maintenance serveur

**Requirements VPS**:
- 16GB RAM minimum
- 50GB disk (pour modÃ¨les)
- GPU recommandÃ© (mais pas obligatoire)
- Docker + Docker Compose

### Option B: Hybride Ollama + Groq

**Architecture**:
```
Bolt Frontend â†’ Groq (GRATUIT, rapide)
BMAD Agents simples â†’ Ollama local (GRATUIT)
BMAD Agents complexes â†’ Groq ou DeepSeek (backup)
```

**Avantages**:
- âœ… Meilleur des 2 mondes
- âœ… Fallback si Ollama down
- âœ… CoÃ»t ~$2-5/mois

---

## ğŸ¯ Recommandation Finale

### Pour MAINTENANT (Dev local):

```yaml
Bolt.DIY:
  provider: Groq
  model: llama-3.3-70b-versatile
  cost: GRATUIT

BMAD Backend:
  provider: DeepSeek
  model: deepseek-chat
  cost: ~$5-10/mois
```

**Total mensuel**: **~$5-10** (vs $200+ avec Claude/OpenAI)

### Pour VPS (Production):

```yaml
Bolt.DIY:
  provider: Groq
  model: llama-3.3-70b-versatile
  cost: GRATUIT
  backup: Ollama local

BMAD Backend:
  provider: Ollama local
  models:
    - llama3.2:3b (conversations simples)
    - qwen2.5-coder:7b (code)
  cost: GRATUIT
  backup: DeepSeek ($5/mois)
```

**Total mensuel**: **$0-5** ğŸ‰

---

## ğŸ“Š Comparaison ScÃ©narios d'Usage

### ScÃ©nario 1: Utilisateur LÃ©ger (10 projets/mois)
| Solution | CoÃ»t |
|----------|------|
| Claude + OpenAI | ~$50-100 |
| Groq + DeepSeek | ~$5 |
| Groq + Ollama | **$0** âœ… |

### ScÃ©nario 2: Utilisateur Moyen (50 projets/mois)
| Solution | CoÃ»t |
|----------|------|
| Claude + OpenAI | ~$200-500 |
| Groq + DeepSeek | ~$15-25 |
| Groq + Ollama | **$0** âœ… |

### ScÃ©nario 3: Production (1000 utilisateurs/mois)
| Solution | CoÃ»t |
|----------|------|
| Claude + OpenAI | ~$10,000+ |
| Groq + DeepSeek | ~$300-500 |
| Groq + Ollama (VPS puissant) | **~$100** (coÃ»t VPS) âœ… |

---

## ğŸ”§ Actions ImmÃ©diates

### 1. Maintenant (5 min):
```bash
# Configure Bolt pour Groq
# 1. Ouvre http://localhost:5174
# 2. Settings â†’ Provider: Groq
# 3. Model: llama-3.3-70b-versatile
# 4. Teste un message
```

### 2. Aujourd'hui (30 min):
```bash
# Teste tous les providers disponibles
# VÃ©rifie les rate limits et performances
```

### 3. Cette semaine (2h):
```bash
# PrÃ©pare configuration Ollama pour VPS
# TÃ©lÃ©charge modÃ¨les optimaux
# Configure fallback Groq/DeepSeek
```

---

## ğŸ“ Fichiers Ã  Modifier

### 1. `backend/rag-compat/app/routers/bmad_chat.py`

Ajouter support Ollama:
```python
def get_ai_client():
    """Get AI client based on config"""
    provider = os.getenv("BMAD_PROVIDER", "deepseek")

    if provider == "ollama":
        return OpenAI(
            api_key="ollama",
            base_url=os.getenv("OLLAMA_API_BASE_URL", "http://ollama:11434/v1")
        ), "llama3.2:3b"
    elif provider == "groq":
        return OpenAI(
            api_key=os.getenv("GROQ_API_KEY"),
            base_url="https://api.groq.com/openai/v1"
        ), "llama-3.3-70b-versatile"
    else:  # deepseek (default)
        return get_deepseek_client(), "deepseek-chat"
```

### 2. `docker-compose.yml`

Ajouter service Ollama:
```yaml
ollama:
  image: ollama/ollama:latest
  container_name: ragdz-ollama
  volumes:
    - ollama_data:/root/.ollama
  ports:
    - "11434:11434"
  networks:
    - ragdz-network
  # Uncomment if you have GPU
  # deploy:
  #   resources:
  #     reservations:
  #       devices:
  #         - driver: nvidia
  #           count: 1
  #           capabilities: [gpu]

volumes:
  ollama_data:
```

### 3. `.env`

Ajouter configuration:
```env
# BMAD Provider Choice
BMAD_PROVIDER=deepseek  # ollama | groq | deepseek
OLLAMA_API_BASE_URL=http://ollama:11434/v1
USE_OLLAMA=false
```

---

## ğŸ‰ Ã‰conomies RÃ©alisÃ©es

Avec configuration **Groq + DeepSeek**:
- **Claude/OpenAI**: $200-500/mois
- **Groq + DeepSeek**: $5-10/mois
- **Ã‰conomie**: **~$190-490/mois** (95-98% moins cher)

Avec configuration **Groq + Ollama** sur VPS:
- **Claude/OpenAI**: $200-500/mois
- **Groq + Ollama**: $0/mois (+ $20-50 coÃ»t VPS)
- **Ã‰conomie**: **~$150-450/mois** (75-90% moins cher)

---

**Auteur**: Claude Code Assistant
**Version**: 1.0
**Date**: 2025-01-20
