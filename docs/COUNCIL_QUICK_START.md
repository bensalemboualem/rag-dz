# LLM Council - Guide de DÃ©marrage Rapide

## ğŸ¯ Qu'est-ce que LLM Council ?

LLM Council est un systÃ¨me innovant qui consulte **plusieurs modÃ¨les d'IA simultanÃ©ment** pour obtenir des rÃ©ponses plus robustes et fiables. Au lieu de se fier Ã  un seul modÃ¨le, le systÃ¨me:

1. **Collecte les opinions** de 3 modÃ¨les d'IA diffÃ©rents
2. **Optionnellement, fait une revue croisÃ©e** oÃ¹ chaque modÃ¨le Ã©value les autres
3. **SynthÃ©tise une rÃ©ponse finale** optimale par un "chairman"

## ğŸš€ Installation Express (5 minutes)

### 1. Mettre Ã  jour .env.local

```bash
# Copier l'exemple si vous n'avez pas encore de .env.local
cp .env.example .env.local

# Ajouter vos clÃ©s API
nano .env.local
```

Configurez au minimum:
```bash
# Pour Claude (chairman)
ANTHROPIC_API_KEY=sk-ant-xxxxx

# Pour Gemini
GOOGLE_GENERATIVE_AI_API_KEY=AIzaSy-xxxxx

# Ollama sera disponible automatiquement via Docker
OLLAMA_BASE_URL=http://iafactory-ollama:11434
```

### 2. DÃ©marrer Ollama

Ollama est dÃ©jÃ  configurÃ© dans docker-compose.yml. Pour le dÃ©marrer:

```bash
# Option 1: DÃ©marrer uniquement Ollama
docker-compose up -d iafactory-ollama

# Option 2: DÃ©marrer toute la stack (recommandÃ©)
docker-compose up -d
```

TÃ©lÃ©charger le modÃ¨le Llama 3:
```bash
docker exec -it iaf-dz-ollama ollama pull llama3:8b
```

### 3. RedÃ©marrer le backend

```bash
docker-compose restart iafactory-backend
```

### 4. Tester l'installation

```bash
# Avec Python
python test-council.py

# Ou avec curl
curl http://localhost:8180/api/council/health
```

## ğŸ“– Utilisation

### Via l'Interface Web

1. Ouvrir http://localhost:8182 (IAFactory Hub)
2. Cliquer sur "LLM Council" dans la navigation
3. Poser votre question
4. Attendre 15-30 secondes pour la rÃ©ponse

### Via l'API

#### Test simple
```bash
curl -X POST http://localhost:8180/api/council/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Quelles sont les meilleures pratiques pour sÃ©curiser une API REST ?",
    "enable_review": false
  }'
```

#### Avec revue croisÃ©e (plus lent)
```bash
curl -X POST http://localhost:8180/api/council/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Comment optimiser les performances PostgreSQL ?",
    "enable_review": true
  }'
```

#### Lister les providers disponibles
```bash
curl http://localhost:8180/api/council/providers
```

## ğŸ”§ Configuration AvancÃ©e

### Changer le chairman

Dans `.env.local`:
```bash
COUNCIL_CHAIRMAN=gemini  # Par dÃ©faut: claude
```

### Activer la revue par dÃ©faut

```bash
COUNCIL_ENABLE_REVIEW=true  # Par dÃ©faut: false
```

### URL Ollama custom

```bash
OLLAMA_BASE_URL=http://mon-ollama:11434
```

## ğŸ“Š Architecture du Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STAGE 1: OPINIONS                  â”‚
â”‚  Claude, Gemini, Ollama rÃ©pondent en parallÃ¨le  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STAGE 2: REVIEW (optionnel)             â”‚
â”‚  Chaque modÃ¨le Ã©value les autres                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STAGE 3: SYNTHESIS                      â”‚
â”‚  Chairman synthÃ©tise la rÃ©ponse finale          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Cas d'usage recommandÃ©s

### âœ… Quand utiliser Council

- **DÃ©cisions importantes** : analyses stratÃ©giques, recommandations
- **Validation technique** : architecture, sÃ©curitÃ©, performance
- **Recherche approfondie** : synthÃ¨se de documentation
- **Questions complexes** : nÃ©cessitant plusieurs perspectives

### âŒ Quand NE PAS utiliser Council

- Questions simples et factuelles
- Besoins de rapiditÃ© (< 5s)
- TÃ¢ches crÃ©atives (gÃ©nÃ©ration de contenu)
- Cas oÃ¹ un seul modÃ¨le suffit

## ğŸ’° CoÃ»ts estimÃ©s

### Mode Standard (sans review)
- **Latence**: 15-30 secondes
- **CoÃ»t par requÃªte**: ~$0.015 (1.5Â¢)
- **Usage**: Questions de complexitÃ© moyenne

### Mode Premium (avec review)
- **Latence**: 30-60 secondes
- **CoÃ»t par requÃªte**: ~$0.030 (3Â¢)
- **Usage**: DÃ©cisions critiques

**Note**: Ollama est gratuit (modÃ¨le local)

## ğŸ› DÃ©pannage

### ProblÃ¨me: "No available providers"

**Solution**: VÃ©rifier vos clÃ©s API dans `.env.local`
```bash
# VÃ©rifier
curl http://localhost:8180/api/council/providers

# Tester la connectivitÃ©
curl -X POST http://localhost:8180/api/council/test
```

### ProblÃ¨me: Ollama non disponible

**Solution**: VÃ©rifier le container Ollama
```bash
# Status du container
docker ps | grep ollama

# Logs
docker logs iaf-dz-ollama

# RedÃ©marrer
docker-compose restart iafactory-ollama

# TÃ©lÃ©charger le modÃ¨le si manquant
docker exec -it iaf-dz-ollama ollama pull llama3:8b
```

### ProblÃ¨me: Timeout

**Solution**: Augmenter les timeouts dans `backend/rag-compat/app/modules/council/config.py`:
```python
STAGE1_TIMEOUT: int = 60  # Au lieu de 30
TOTAL_TIMEOUT: int = 180  # Au lieu de 90
```

### ProblÃ¨me: Erreur Claude ou Gemini

**Solution**: VÃ©rifier les clÃ©s API et les quotas
```bash
# Test manuel Claude
curl https://api.anthropic.com/v1/messages \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "content-type: application/json" \
  -d '{"model":"claude-sonnet-4-20250514","max_tokens":100,"messages":[{"role":"user","content":"test"}]}'

# Test manuel Gemini
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?key=$GOOGLE_GENERATIVE_AI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"contents":[{"parts":[{"text":"test"}]}]}'
```

## ğŸ“š Endpoints API

| Endpoint | MÃ©thode | Description |
|----------|---------|-------------|
| `/api/council/query` | POST | Interroger le Council |
| `/api/council/providers` | GET | Liste des providers |
| `/api/council/test` | POST | Tester la connectivitÃ© |
| `/api/council/config` | GET | Configuration actuelle |
| `/api/council/health` | GET | SantÃ© du service |

## ğŸ”— Documentation complÃ¨te

- **API Docs**: http://localhost:8180/docs#/Council
- **Architecture**: [ARCHITECTURE.md](../ARCHITECTURE.md)
- **Tests**: `python test-council.py`

## ğŸ“ Support

Pour la dÃ©mo du 6 dÃ©cembre avec AlgÃ©rie TÃ©lÃ©com:
- Questions prÃ©parÃ©es dans `docs/DEMO_COUNCIL_DZ.md`
- Pricing dÃ©taillÃ© dans `docs/COUNCIL_PRICING.md`

## ğŸ‰ Checklist de validation

- [ ] Backend dÃ©marrÃ© avec Council activÃ©
- [ ] Les 3 providers affichent "available: true"
- [ ] Test simple rÃ©ussit (< 30s)
- [ ] Interface web accessible sur /council
- [ ] Documentation API visible sur /docs
