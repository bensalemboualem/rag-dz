# üéØ Council Personnalisable - Guide d'Upgrade

## ‚úÖ Ce qui a √©t√© impl√©ment√©

Vous avez maintenant un syst√®me Council **compl√®tement personnalisable** avec :

### üèóÔ∏è Backend
1. **`models_config.py`** - Catalogue de tous les LLMs disponibles (10+ mod√®les)
   - OpenAI: GPT-4 Turbo, GPT-3.5 Turbo
   - Anthropic: Claude Opus 4, Claude Sonnet 4, Claude Sonnet 3.5
   - Google: Gemini 1.5 Pro, Gemini 1.5 Flash
   - Mistral: Mistral Large
   - Ollama (local): Llama3 70B/8B, Mixtral, CodeLlama

2. **`universal_provider.py`** - Factory universel pour tous les providers
   - Support multi-providers automatique
   - Gestion des erreurs robuste
   - D√©tection de disponibilit√©

3. **`flexible_orchestrator.py`** - Orchestrateur acceptant toute combinaison
   - 3 experts + 1 chairman personnalisables
   - Review crois√©e optionnelle
   - Estimation co√ªts/temps en temps r√©el

4. **`council_custom.py`** - API endpoints pour la personnalisation
   - `GET /api/council/models/all` - Liste tous les mod√®les
   - `GET /api/council/presets` - Configurations recommand√©es
   - `POST /api/council/estimate` - Estimation avant ex√©cution
   - `POST /api/council/custom-query` - Ex√©cution personnalis√©e

### üé® Frontend
- **`council-custom.html`** - Interface compl√®te avec dropdowns
   - S√©lection ind√©pendante des 3 experts + chairman
   - 6 presets recommand√©s (Balanced, Premium, Economy, Fast, Multilingual, Code)
   - Estimation dynamique co√ªt/temps
   - Warning si config trop co√ªteuse
   - Affichage forces de chaque mod√®le

## üöÄ Comment tester

### 1. D√©marrer le backend
```bash
cd backend/rag-compat
docker-compose up backend
# ou
python -m uvicorn app.main:app --reload --port 8180
```

### 2. D√©marrer le serveur Council
```bash
node council-server.js
```

### 3. Acc√©der aux interfaces

#### Version Standard (config fixe)
```
http://localhost:3000/
```

#### Version Custom (personnalisable) ‚≠ê NOUVEAU
```
http://localhost:3000/custom
```

## üéØ Utilisation

### Preset "√âquilibr√©" (recommand√© pour d√©marrer)
- Expert 1: Claude Sonnet 3.5
- Expert 2: Gemini 1.5 Pro
- Expert 3: Llama 3 70B (local, gratuit)
- Chairman: Claude Sonnet 3.5
- **Co√ªt**: ~400-500 DZD/requ√™te
- **Temps**: 15-25s

### Preset "Premium" (maximum qualit√©)
- Expert 1: Claude Opus 4
- Expert 2: GPT-4 Turbo
- Expert 3: Gemini 1.5 Pro
- Chairman: Claude Opus 4
- **Co√ªt**: ~2000 DZD/requ√™te
- **Temps**: 20-30s

### Preset "Economy" (100% gratuit local)
- Expert 1: Llama 3 70B
- Expert 2: Mixtral 8x7B
- Expert 3: Llama 3 8B
- Chairman: Llama 3 70B
- **Co√ªt**: 0 DZD (local)
- **Temps**: 15-20s (si Ollama configur√©)

## üîë Configuration requise

### Variables d'environnement

```bash
# Dans .env.local ou .env

# OpenAI (GPT models)
OPENAI_API_KEY=sk-...

# Anthropic (Claude models)
ANTHROPIC_API_KEY=sk-ant-...

# Google (Gemini models)
GOOGLE_API_KEY=AIza...

# Mistral
MISTRAL_API_KEY=...

# Ollama (local - optionnel mais recommand√©)
OLLAMA_BASE_URL=http://localhost:11434
```

### Installer Ollama (optionnel, pour mod√®les locaux gratuits)

```bash
# Windows / Mac / Linux
# T√©l√©charger depuis: https://ollama.ai

# Puis installer les mod√®les
ollama pull llama3:70b
ollama pull llama3:8b
ollama pull mixtral:8x7b
ollama pull codellama:34b
```

## üí° Exemples de configurations par cas d'usage

### Pour l'Alg√©rie T√©l√©com (√©quilibr√© co√ªt/qualit√©)
- Claude Sonnet 3.5 (rapide, pr√©cis)
- Gemini Pro (√©conomique, multimodal)
- Llama3 70B (gratuit, local, souverain)
- **‚Üí ~500 DZD/requ√™te**

### Pour client suisse premium
- Claude Opus 4 (meilleure qualit√©)
- GPT-4 Turbo (expertise technique)
- Gemini Pro (multimodal)
- **‚Üí ~2000 DZD / 15 CHF**

### Pour d√©veloppement logiciel
- GPT-4 Turbo (code expert)
- CodeLlama 34B (sp√©cialis√© code)
- Claude Opus 4 (architecture)
- **‚Üí Focus code, debug, architecture**

### Pour multilingue FR/AR
- Mistral Large (fran√ßais natif)
- Claude Sonnet 3.5 (multilingue)
- Llama3 70B (flexible)
- **‚Üí Optimis√© fran√ßais et arabe**

## üìä API Endpoints disponibles

### Liste tous les mod√®les
```bash
curl http://localhost:8180/api/council/models/all
```

### Estimation de co√ªt
```bash
curl -X POST http://localhost:8180/api/council/estimate \
  -H "Content-Type: application/json" \
  -d '{
    "expert1": "claude-sonnet-3.5",
    "expert2": "gemini-1.5-pro",
    "expert3": "llama3-70b",
    "chairman": "claude-sonnet-3.5",
    "enable_review": false
  }'
```

### Ex√©cution custom
```bash
curl -X POST http://localhost:8180/api/council/custom-query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Expliquez la diff√©rence entre authentification et autorisation",
    "expert1": "claude-sonnet-3.5",
    "expert2": "gemini-1.5-pro",
    "expert3": "gpt-4-turbo",
    "chairman": "claude-opus-4",
    "enable_review": false
  }'
```

## üéÅ Valeur ajout√©e vs version de base

| Feature | Version de base | Version Custom |
|---------|-----------------|----------------|
| Config | Fixe (claude/gemini/ollama) | **Dynamique (10+ mod√®les)** ‚úÖ |
| Choix | Automatique | **Manuel par l'utilisateur** ‚úÖ |
| Presets | Aucun | **6 configurations recommand√©es** ‚úÖ |
| Estimation | Non | **Co√ªt & temps avant ex√©cution** ‚úÖ |
| Providers | 3 | **5 (OpenAI, Anthropic, Google, Mistral, Ollama)** ‚úÖ |
| Local | Ollama uniquement | **4 mod√®les locaux gratuits** ‚úÖ |

## üö® Troubleshooting

### "Mod√®le indisponible" dans le dropdown
‚Üí V√©rifiez que la cl√© API correspondante est dans `.env.local`

### Ollama ne r√©pond pas
```bash
# V√©rifier qu'Ollama est lanc√©
ollama list

# Sur Windows, lancer Ollama Desktop
# Ou: ollama serve
```

### Erreur "Module council_custom not found"
```bash
# Red√©marrer le backend
cd backend/rag-compat
docker-compose restart backend
```

## üìà Prochaines √©tapes possibles

1. **Sauvegarde de configs** - Permettre de sauvegarder ses combinaisons favorites
2. **Mode comparaison** - Afficher les 3 opinions c√¥te √† c√¥te
3. **Historique** - Garder l'historique des requ√™tes et co√ªts
4. **Multi-chairman** - Permettre plusieurs synth√®ses (consensus, vote, etc.)
5. **Templates** - Cr√©er des templates de questions par domaine

## ‚úÖ R√©sum√©

Vous avez maintenant un **Council compl√®tement personnalisable** qui vous diff√©rencie de la version Karpathy :

- ‚úÖ 10+ mod√®les LLM support√©s
- ‚úÖ S√©lection manuelle des experts
- ‚úÖ Estimation co√ªts/temps en temps r√©el
- ‚úÖ 6 presets recommand√©s
- ‚úÖ Interface intuitive avec dropdowns
- ‚úÖ Support local gratuit (Ollama)
- ‚úÖ API compl√®te pour int√©gration

**C'est votre USP ! Aucun service concurrent n'offre cette flexibilit√©.**
